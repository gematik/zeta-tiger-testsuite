"""Traceability helpers for the testsuite documentation.

This module aggregates requirements, test aspects, feature metadata and optional
execution results into a collection of artefacts consumed by the Asciidoc
manual. The default configuration focuses on the ``gemSpec_ZETA`` specification,
which forms the canonical requirements baseline for the project.

Entry points deliberately avoid side effects unless ``write_outputs`` is
explicitly enabled so that higher-level tools can compose the functionality
without touching the filesystem during dry runs.
"""

from __future__ import annotations

import csv
import io
import json
import logging
import re
from collections import defaultdict
from dataclasses import asdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Sequence, Set, Tuple

from pytablewriter import AsciiDocTableWriter

from .models import (
  Requirement,
  ScenarioCoverage,
  TestAspect,
  TraceabilityLink,
  TraceabilityRecord,
  TraceabilityReport,
  UseCase,
)

LOGGER = logging.getLogger(__name__)

AUTOGENERATED_DISCLAIMER = (
  "Autogenerated by testsuite-traceability; do not edit manually."
)
MERMAID_DISCLAIMER = f"%% {AUTOGENERATED_DISCLAIMER}"
ASCIIDOC_DISCLAIMER = f"// {AUTOGENERATED_DISCLAIMER}"

# Common Mermaid configuration fragments.
MERMAID_THEME_DIRECTIVE = (
  "%%{init: {'theme': 'base', 'themeVariables': {"
  "'primaryColor': '#0057B8',"
  "'primaryTextColor': '#ffffff',"
  "'primaryBorderColor': '#003f8a',"
  "'secondaryColor': '#E8F1FA',"
  "'secondaryBorderColor': '#004f9f',"
  "'lineColor': '#0057B8',"
  "'textColor': '#0F1F2E',"
  "'fontFamily': 'Arial, Helvetica, sans-serif',"
  "'pie1': '#0057B8',"
  "'pie2': '#F58220',"
  "'pie3': '#C8102E',"
  "'pie4': '#7A6A9C',"
  "'pie5': '#008060'"
  "}}}%%"
)

# Requirement sources bundled into the documentation. Primary catalogue is
# gemSpec_ZETA; when set to None, load all subfolders under afos.
REQUIREMENT_SPEC_SUBDIR = None

# Regex helpers for parsing adoc / gherkin files.
REQ_TITLE_PATTERN = re.compile(
    r"^====\s+(?P<id>[A-Za-z0-9_\-]+)\s*-\s*(?P<title>.+?)\s*$")
TA_TITLE_PATTERN = re.compile(
    r"^=====\s+(?P<id>TA_[A-Z0-9_]+)\s*-\s*(?P<title>.+?)\s*$")
ANCHOR_PATTERN = re.compile(r"^\[#(?P<anchor>[A-Za-z0-9_\-]+)]")
FEATURE_NAME_PATTERN = re.compile(r"^Funktionalität:\s*(?P<name>.+?)\s*$",
                                  re.IGNORECASE)
SCENARIO_PATTERN = re.compile(
    r"^Szenario(?:grundriss)?(?::|\s+Outline:)\s*(?P<name>.+?)\s*$",
    re.IGNORECASE)
TAG_PATTERN = re.compile(r"@([A-Za-z0-9_\-]+)")
PRODUCT_NOT_IMPLEMENTED_TAGS = {
  "product_not_impl",
  "produkt_not_impl",
  "not_impl",
  "not_implemented",
  "canary",
}


def build_traceability(
    project_root: Optional[Path] = None,
    cucumber_json: Optional[Path] = None,
    product_status_csv: Optional[Path] = None,
    *,
    write_outputs: bool = True,
) -> TraceabilityReport:
  """Collect traceability data and optionally persist the derived artefacts.

  The routine focuses on the ``gemSpec_ZETA`` catalogue, ensuring that counts
  such as the "total number of requirements" align with the specification that
  is actually in scope for the testsuite.

  Args:
      project_root: Repository root override. When omitted the path is derived
        from the location of this module.
      cucumber_json: Optional path to a Cucumber JSON report with execution
        metadata. Missing files are treated as an empty result set.
      write_outputs: When ``True`` the generated tables, diagrams and JSON
        payload are written to the standard documentation targets.
      product_status_csv: Optional CSV with product implementation flags
        (defaults to ``docs/asciidoc/tables/product_implementation.csv``).
  """

  root = project_root or Path(__file__).resolve().parents[5]
  docs_root = root / "docs"
  asciidoc_root = docs_root / "asciidoc"

  if REQUIREMENT_SPEC_SUBDIR:
    requirements_dirs = [asciidoc_root / "afos" / REQUIREMENT_SPEC_SUBDIR]
    test_aspects_dirs = [
      asciidoc_root / "testaspekte" / REQUIREMENT_SPEC_SUBDIR
    ]
  else:
    requirements_dirs = [
      path for path in (asciidoc_root / "afos").iterdir() if path.is_dir()
      and any(adoc for adoc in path.glob("**/*.adoc")
              if adoc.name.lower() != "readme.adoc")
    ]
    test_aspects_dirs = [
      path for path in (asciidoc_root / "testaspekte").iterdir()
      if path.is_dir() and any(path.glob("**/TA_*.adoc"))
    ]

  if not requirements_dirs:
    raise FileNotFoundError(
        f"No requirement catalogues found under {asciidoc_root / 'afos'}")
  if not test_aspects_dirs:
    raise FileNotFoundError(
        f"No test aspect catalogues found under {asciidoc_root / 'testaspekte'}"
    )
  diagrams_dir = asciidoc_root / "diagrams"
  tables_dir = asciidoc_root / "tables"
  features_root = root / "src" / "test" / "resources" / "features"
  if write_outputs:
    diagrams_dir.mkdir(parents=True, exist_ok=True)
    tables_dir.mkdir(parents=True, exist_ok=True)

  requirements = {}
  for req_dir in requirements_dirs:
    requirements.update(_load_requirements(req_dir))
  test_aspects = {}
  for ta_dir in test_aspects_dirs:
    test_aspects.update(_load_test_aspects(ta_dir, requirements))

  user_story_anchors = _load_user_story_anchors(features_root)
  use_case_anchors = _load_use_case_anchors(features_root)

  use_cases, scenarios = _parse_feature_files(
      features_root,
      user_story_anchors=user_story_anchors,
      use_case_anchors=use_case_anchors,
  )

  execution_sources = _resolve_execution_sources(root, cucumber_json)
  execution_results = _load_execution_results(execution_sources,
                                              use_case_anchors)

  records = _build_traceability_records(
      requirements=requirements,
      test_aspects=test_aspects,
      use_cases=use_cases,
      scenarios=scenarios,
      execution_results=execution_results,
  )

  status_path = product_status_csv if product_status_csv is not None else (
      tables_dir / "product_implementation.csv")
  if not status_path.is_absolute():
    status_path = (root / status_path).resolve()
  product_status = _load_product_status(status_path)
  status_path_for_report: Optional[Path] = None
  try:
    status_path_for_report = status_path.relative_to(root)
  except ValueError:
    status_path_for_report = Path(status_path.name)

  known_user_stories = {
    story
    for story in (
        set(user_story_anchors.values()) | {uc.user_story_id for uc in
                                            use_cases.values()}
    )
    if story
  }
  features_table = _render_features_table(
      use_cases, records, sorted(known_user_stories), product_status)
  product_gap_table, gap_summary = _render_product_gap_table(
      requirements,
      test_aspects,
      records,
      product_status,
      status_path_for_report,
  )
  gap_summary_chart, gap_summary_table = _render_product_gap_summary(
      gap_summary)
  traceability_table = _render_traceability_matrix(records)
  mermaid_diagram = _render_mermaid_diagram(records, requirements, test_aspects,
                                            use_cases)
  coverage_charts, coverage_summary = _render_coverage_charts(
      records, requirements, test_aspects
  )
  humanized_last_success = _humanize_timestamps(records, "last_success")
  humanized_last_run = _humanize_timestamps(records, "last_run")

  traceability_links = [
    TraceabilityLink(
        requirement=record.requirement_id,
        test_aspect=record.test_aspect_id,
        use_case=record.use_case_id,
        implemented=record.implemented,
        product_implemented=record.product_implemented,
        last_run=_format_timestamp_iso(record.last_run),
        last_run_human=humanized_last_run.get(
            (record.test_aspect_id, record.use_case_id or "")
        ),
        last_run_status=record.last_run_status,
        last_success=_format_timestamp_iso(record.last_success),
        last_success_human=humanized_last_success.get(
            (record.test_aspect_id, record.use_case_id or "")
        ),
        scenarios=sorted(record.scenario_names),
    )
    for record in records
  ]

  report = TraceabilityReport(
      generated_at=_format_timestamp_iso(datetime.utcnow()),
      requirements={
        req_id: _dataclass_to_payload(req, path_fields=("source",))
        for req_id, req in requirements.items()
      },
      test_aspects={
        ta_id: _dataclass_to_payload(ta, path_fields=("source",))
        for ta_id, ta in test_aspects.items()
      },
      use_cases={
        anchor: {
          "tag_id": uc.tag_id,
          "anchor_id": uc.anchor_id,
          "title": uc.title,
          "user_story_id": uc.user_story_id,
          "feature_files": [
            str(path.relative_to(root).as_posix()) for path in uc.feature_files
          ],
        }
        for anchor, uc in sorted(use_cases.items())
      },
      traceability=traceability_links,
      coverage_summary=coverage_summary,
  )

  if write_outputs:
    _write_text_file(tables_dir / "features_table.adoc", features_table)
    _write_text_file(tables_dir / "product_gap_table.adoc", product_gap_table)
    _write_text_file(tables_dir / "product_gap_summary.adoc",
                     gap_summary_table)
    _write_text_file(tables_dir / "traceability_matrix.adoc",
                     traceability_table)
    _write_text_file(diagrams_dir / "traceability-overview.mmd",
                     mermaid_diagram)
    _write_text_file(diagrams_dir / "product-gap-summary.mmd",
                     gap_summary_chart)
    for filename, content in coverage_charts.items():
      _write_text_file(diagrams_dir / filename, content)

    generated_dir = root / "target" / "generated-docs"
    generated_dir.mkdir(parents=True, exist_ok=True)
    _write_text_file(
        generated_dir / "traceability.json",
        json.dumps(asdict(report), indent=2, ensure_ascii=False),
    )

  return report


def _load_requirements(directory: Path) -> Dict[str, Requirement]:
  """Load requirement definitions for the configured specification folder.

  The loader scans recursively for ``*.adoc`` files (excluding readmes) within
  ``directory`` and returns a mapping from requirement identifier to
  :class:`Requirement` instances. Filenames become IDs.
  """
  requirements: Dict[str, Requirement] = {}
  for path in sorted(directory.rglob("*.adoc")):
    if path.name.lower() == "readme.adoc":
      continue
    requirement_id = path.stem
    title = _extract_title(path, REQ_TITLE_PATTERN, fallback=requirement_id)
    requirements[requirement_id] = Requirement(requirement_id, title, path)
  return requirements


def _load_test_aspects(
    directory: Path,
    requirements: Dict[str, Requirement],
) -> Dict[str, TestAspect]:
  """Load test aspect definitions and relate them to their parent requirement.

  Each ``TA_*.adoc`` file is parsed for its title and linked to the
  requirement folder it resides in.  Any aspect that references an unknown
  requirement triggers a warning, helping authors keep the catalogues in sync.
  """
  test_aspects: Dict[str, TestAspect] = {}
  for path in sorted(directory.rglob("TA_*.adoc")):
    if path.name.lower() == "readme.adoc":
      continue
    test_aspect_id = path.stem
    requirement_id = path.parent.name
    title = _extract_title(path, TA_TITLE_PATTERN, fallback=test_aspect_id)
    if requirement_id not in requirements:
      LOGGER.warning(
          "Test aspect %s references unknown requirement %s (%s)",
          test_aspect_id,
          requirement_id,
          path,
      )
    test_aspects[test_aspect_id] = TestAspect(test_aspect_id, title,
                                              requirement_id, path)
  return test_aspects


def _load_user_story_anchors(userstories_root: Path) -> Dict[str, str]:
  """Return anchor IDs for all user story overview files.

  The resulting dictionary maps the directory name (``UserStory_01``) to the
  anchor declared in the corresponding ``readme.adoc``.  Missing directories are
  tolerated to keep the generator robust on incomplete checkouts.
  """
  anchors: Dict[str, str] = {}
  if not userstories_root.exists():
    return anchors
  for readme in userstories_root.rglob("UserStory_*/readme.adoc"):
    anchor = _extract_anchor(readme)
    if anchor:
      anchors.setdefault(readme.parent.name, anchor)
  return anchors


def _load_use_case_anchors(userstories_root: Path) -> Dict[str, str]:
  """Retrieve anchors for use-case documentation snippets.

  Both the directory name (``UseCase_01``) and the combined tag
  (``UserStory_01/UseCase_01`` expressed as ``UseCase_01_01``) are mapped to the
  resolved anchor so feature files and traceability reports may reference
  whichever convention is more convenient.
  """
  anchors: Dict[str, str] = {}
  if not userstories_root.exists():
    return anchors
  for usecase_readme in userstories_root.rglob("UseCase_*/readme.adoc"):
    anchor = _extract_anchor(usecase_readme)
    if anchor:
      use_case_dir = usecase_readme.parent.name
      anchors[use_case_dir] = anchor
      story_dir = usecase_readme.parent.parent.name if usecase_readme.parent.parent else ""
      combined_tag = _build_use_case_tag(story_dir, use_case_dir)
      if combined_tag and combined_tag not in anchors:
        anchors[combined_tag] = anchor
  return anchors


def _load_product_status(path: Path) -> Dict[str, Optional[str]]:
  """Read product implementation status from CSV.

  Expected columns: ``Anforderung`` (ID), ``umgesetzt`` (``ja``/``nein``/
  ``teilweise``). Unrecognised values are treated as unknown.
  """
  if not path.exists():
    LOGGER.info("Produktstatus-Datei %s nicht gefunden, verwende leere Angaben",
                path)
    return {}

  records: Dict[str, Optional[str]] = {}
  try:
    with path.open(encoding="utf-8", newline="") as handle:
      reader = csv.DictReader(handle)
      for row in reader:
        requirement_id = (
            row.get("Anforderung") or row.get("anforderung")
            or row.get("requirement_id") or row.get("id") or "").strip()
        if not requirement_id:
          continue
        implemented_raw = (
            row.get("umgesetzt") or row.get("implemented") or row.get("status")
            or "").strip()
        records[requirement_id] = _normalise_product_flag(implemented_raw)
  except (OSError, csv.Error) as exc:
    LOGGER.warning("Konnte Produktstatus nicht aus %s laden: %s", path, exc)
    return {}

  LOGGER.info("Produktstatus aus %s geladen (%d Einträge)", path, len(records))
  return records


def _parse_feature_files(
    features_root: Path,
    *,
    user_story_anchors: Dict[str, str],
    use_case_anchors: Dict[str, str],
) -> Tuple[Dict[str, UseCase], List[ScenarioCoverage]]:
  """Iterate over feature files and derive coverage information.

  The parser collects metadata on two levels:

  * :class:`UseCase` instances representing the feature-level groupings,
    populated via tags or directory inference.
  * :class:`ScenarioCoverage` entries capturing individual scenarios,
    including their tagged requirements and test aspects.

  Both structures are later combined with the requirement catalogues to build
  tables and diagrams.
  """
  use_cases: Dict[str, UseCase] = {}
  scenarios: List[ScenarioCoverage] = []

  for feature_path in sorted(features_root.rglob("*.feature")):
    feature_rel = feature_path.relative_to(features_root)
    story_key = _story_key_from_path(feature_rel)
    user_story_id = user_story_anchors.get(story_key,
                                           story_key or "UNSPECIFIED")

    file_content = feature_path.read_text(encoding="utf-8").splitlines()
    feature_tags: Set[str] = set()
    pending_tags: List[str] = []
    feature_name: Optional[str] = None

    for raw_line in file_content:
      stripped = raw_line.strip()
      if not stripped:
        continue
      if stripped.startswith("@"):
        pending_tags.extend(TAG_PATTERN.findall(stripped))
        continue
      feature_match = FEATURE_NAME_PATTERN.match(stripped)
      if feature_match:
        feature_name = feature_match.group("name").strip()
        feature_tags = set(pending_tags)
        pending_tags = []
        use_case_tags = {tag for tag in feature_tags if
                         tag.startswith("UseCase")}
        if not use_case_tags:
          inferred_use_case = _infer_use_case_from_path(feature_rel)
          if inferred_use_case:
            use_case_tags = {inferred_use_case}
        for tag in use_case_tags:
          anchor_id = use_case_anchors.get(tag, tag)
          use_case = use_cases.setdefault(
              anchor_id,
              UseCase(
                  tag_id=tag,
                  anchor_id=anchor_id,
                  title=feature_name or tag,
                  user_story_id=user_story_id,
              ),
          )
          if feature_path not in use_case.feature_files:
            use_case.feature_files.append(feature_path)
        continue

      scenario_match = SCENARIO_PATTERN.match(stripped)
      if scenario_match:
        scenario_tags = set(pending_tags)
        pending_tags = []
        scenario_name = scenario_match.group("name").strip()
        tags = feature_tags | scenario_tags
        product_implemented = not _has_not_implemented_tag(tags)

        use_case_tags = {tag for tag in tags if tag.startswith("UseCase")}
        if not use_case_tags:
          inferred = _infer_use_case_from_path(feature_rel)
          if inferred:
            use_case_tags = {inferred}

        test_aspect_tags = {tag for tag in tags if tag.startswith("TA_")}
        requirement_tags = {tag for tag in tags if re.match(r"A_\d+", tag)}

        if not use_case_tags:
          LOGGER.debug(
              "Skipping scenario without UseCase tag %s in %s", scenario_name,
              feature_rel
          )
          continue

        anchor_ids = set()
        for tag in use_case_tags:
          anchor_id = use_case_anchors.get(tag, tag)
          anchor_ids.add(anchor_id)
          use_case = use_cases.setdefault(
              anchor_id,
              UseCase(
                  tag_id=tag,
                  anchor_id=anchor_id,
                  title=feature_name or tag,
                  user_story_id=user_story_id,
              ),
          )
          if feature_path not in use_case.feature_files:
            use_case.feature_files.append(feature_path)

        scenarios.append(
            ScenarioCoverage(
                scenario_name=scenario_name,
                feature=feature_path,
                use_cases=anchor_ids,
                test_aspects=test_aspect_tags,
                requirements=requirement_tags,
                product_implemented=product_implemented,
            )
        )

    if pending_tags:
      LOGGER.debug("Unconsumed tags at end of %s: %s", feature_rel,
                   pending_tags)

  return use_cases, scenarios


def _load_execution_results(
    sources: Sequence[Path],
    use_case_anchors: Dict[str, str],
) -> Dict[Tuple[str, str], Dict[str, object]]:
  """Return execution metadata per TA/use-case (status + timestamps)."""

  if not sources:
    LOGGER.info(
        "No execution artefacts configured; skipping execution metadata.")
    return {}

  results: Dict[Tuple[str, str], Dict[str, object]] = {}
  processed_paths: Set[Path] = set()

  for source in sources:
    if not source.exists():
      LOGGER.debug("Execution artefact %s does not exist, skipping.", source)
      continue

    json_paths: List[Path]
    if source.is_dir():
      json_paths = sorted(source.glob("*.json"))
    else:
      json_paths = [source]

    for json_path in json_paths:
      resolved_json = json_path.resolve()
      if resolved_json in processed_paths:
        continue
      processed_paths.add(resolved_json)
      try:
        payload = json.loads(json_path.read_text(encoding="utf-8"))
      except (json.JSONDecodeError, OSError) as exc:
        LOGGER.warning("Failed to parse %s: %s", json_path, exc)
        continue

      file_mtime = datetime.fromtimestamp(json_path.stat().st_mtime)

      if isinstance(payload, list):
        _merge_cucumber_payload(payload, use_case_anchors, results, file_mtime)
      elif isinstance(payload, dict):
        _merge_serenity_payload(payload, use_case_anchors, results, file_mtime)

  return results


def _merge_cucumber_payload(
    payload: Sequence[dict],
    use_case_anchors: Dict[str, str],
    results: Dict[Tuple[str, str], Dict[str, object]],
    fallback_timestamp: datetime,
) -> None:
  """Merge standard Cucumber JSON data into the result map."""

  for feature in payload:
    for element in feature.get("elements", []):
      tags = {tag.get("name", "").lstrip("@") for tag in
              element.get("tags", [])}
      use_case_tags = {tag for tag in tags if tag.startswith("UseCase")}
      test_aspect_tags = {tag for tag in tags if tag.startswith("TA_")}
      if not use_case_tags or not test_aspect_tags:
        continue

      steps = element.get("steps", [])
      status = "passed"
      for step in steps:
        result = step.get("result", {})
        step_status = result.get("status")
        if step_status is None:
          continue
        if step_status == "skipped" and status == "passed":
          status = "skipped"
        elif step_status != "passed":
          status = step_status
          break

      timestamp = _parse_execution_timestamp(element.get("start_timestamp"),
                                             fallback_timestamp)

      for use_case in use_case_tags:
        anchor = use_case_anchors.get(use_case, use_case)
        for test_aspect in test_aspect_tags:
          key = (test_aspect, anchor)
          _merge_execution_result(results, key, status, timestamp)


def _merge_serenity_payload(
    payload: Dict[str, object],
    use_case_anchors: Dict[str, str],
    results: Dict[Tuple[str, str], Dict[str, object]],
    fallback_timestamp: datetime,
) -> None:
  """Merge Serenity JSON data into the result map."""

  tags = payload.get("tags")
  if not isinstance(tags, list):
    return

  tag_names = {
    str(tag.get("name", ""))
    for tag in tags
    if isinstance(tag, dict) and tag.get("name")
  }

  use_case_tags = {tag for tag in tag_names if tag.startswith("UseCase")}
  test_aspect_tags = {tag for tag in tag_names if tag.startswith("TA_")}
  if not use_case_tags or not test_aspect_tags:
    return

  result_value = str(payload.get("result", "")).lower()
  if result_value in {"success", "passed"}:
    status = "passed"
  elif result_value in {"pending", "skipped"}:
    status = "skipped"
  elif result_value:
    status = "failed"
  else:
    status = "unknown"

  timestamp = _parse_execution_timestamp(
      payload.get("endTime") or payload.get("startTime"),
      fallback_timestamp,
  )

  for use_case in use_case_tags:
    anchor = use_case_anchors.get(use_case, use_case)
    for test_aspect in test_aspect_tags:
      key = (test_aspect, anchor)
      _merge_execution_result(results, key, status, timestamp)


def _merge_execution_result(
    results: Dict[Tuple[str, str], Dict[str, object]],
    key: Tuple[str, str],
    status: str,
    timestamp: datetime,
) -> None:
  """Update the aggregated execution result for a TA/use-case pair."""

  normalised_status = status or "unknown"
  entry = results.setdefault(
      key, {
        "last_run": None,
        "last_run_status": "unknown",
        "last_success": None,
      })

  last_run: Optional[datetime] = entry["last_run"]
  last_success: Optional[datetime] = entry["last_success"]

  if last_run is None or timestamp >= last_run:
    entry["last_run"] = timestamp
    entry["last_run_status"] = normalised_status

  if normalised_status == "passed":
    if last_success is None or timestamp >= last_success:
      entry["last_success"] = timestamp


def _parse_execution_timestamp(raw_value: Optional[str],
    fallback: datetime) -> datetime:
  """Parse ISO-like timestamps with optional zone IDs and nanoseconds."""

  if not raw_value:
    return fallback

  cleaned = raw_value.split("[", 1)[0]
  if cleaned.endswith("Z"):
    cleaned = cleaned[:-1] + "+00:00"

  if "." in cleaned:
    prefix, rest = cleaned.split(".", 1)
    tz_pos = max(rest.rfind("+"), rest.rfind("-"))
    if tz_pos > 0:
      fraction = rest[:tz_pos]
      timezone = rest[tz_pos:]
    else:
      fraction = rest
      timezone = ""
    fraction = (fraction[:6]).ljust(6, "0")
    cleaned = f"{prefix}.{fraction}{timezone}"

  try:
    return datetime.fromisoformat(cleaned)
  except ValueError:
    LOGGER.debug("Failed to parse timestamp %s, falling back to file time",
                 raw_value)
    return fallback


def _build_traceability_records(
    *,
    requirements: Dict[str, Requirement],
    test_aspects: Dict[str, TestAspect],
    use_cases: Dict[str, UseCase],
    scenarios: Sequence[ScenarioCoverage],
    execution_results: Dict[Tuple[str, str], Dict[str, object]],
) -> List[TraceabilityRecord]:
  """Combine static metadata and runtime results into traceability records."""
  ta_to_usecases: Dict[str, Set[str]] = defaultdict(set)
  ta_uc_to_scenarios: Dict[Tuple[str, str], Set[str]] = defaultdict(set)
  ta_uc_product_flags: Dict[Tuple[str, str], bool] = {}

  for scenario in scenarios:
    for ta in scenario.test_aspects or {""}:
      if not ta:
        continue
      for uc in scenario.use_cases:
        ta_to_usecases[ta].add(uc)
        ta_uc_to_scenarios[(ta, uc)].add(scenario.scenario_name)
        key = (ta, uc)
        current = ta_uc_product_flags.get(key, True)
        ta_uc_product_flags[key] = current and scenario.product_implemented

  records: List[TraceabilityRecord] = []

  for ta_id, test_aspect in sorted(test_aspects.items()):
    requirement_id = test_aspect.requirement_id
    use_case_ids = sorted(ta_to_usecases.get(ta_id, set()))

    if not use_case_ids:
      records.append(
          TraceabilityRecord(
              requirement_id=requirement_id,
              test_aspect_id=ta_id,
              use_case_id=None,
              implemented=False,
              product_implemented=False,
              last_run=None,
              last_run_status=None,
              last_success=None,
              scenario_names=set(),
          )
      )
      continue

    for use_case_id in use_case_ids:
      exec_entry = execution_results.get((ta_id, use_case_id), {})
      records.append(
          TraceabilityRecord(
              requirement_id=requirement_id,
              test_aspect_id=ta_id,
              use_case_id=use_case_id,
              implemented=True,
              product_implemented=ta_uc_product_flags.get(
                  (ta_id, use_case_id), True),
              last_run=exec_entry.get("last_run"),
              last_run_status=exec_entry.get("last_run_status"),
              last_success=exec_entry.get("last_success"),
              scenario_names=ta_uc_to_scenarios.get((ta_id, use_case_id),
                                                    set()),
          )
      )

  # Ensure records are grouped by requirement -> test aspect -> use case
  records.sort(
      key=lambda record: (
        record.requirement_id,
        record.test_aspect_id,
        record.use_case_id or "",
      )
  )
  return records


def _render_features_table(
    use_cases: Dict[str, UseCase],
    records: Sequence[TraceabilityRecord],
    user_story_ids: Sequence[str],
    product_status: Dict[str, Optional[str]],
) -> str:
  """Render the user-story to test-aspect table as Asciidoc.

  The product column respects feedback from the product CSV (``ja``/``nein``/
  ``teilweise``); otherwise it remains ``unbekannt`` instead of assuming
  availability.
  """
  user_story_to_usecases: Dict[str, List[UseCase]] = defaultdict(list)
  ta_usage_by_usecase: Dict[str, Dict[str, Dict[str, object]]] = defaultdict(
      dict)

  for record in records:
    if record.use_case_id and record.implemented:
      entry = ta_usage_by_usecase[record.use_case_id].setdefault(
          record.test_aspect_id, {
              "implemented": False,
              "last_run": None,
              "last_run_status": None,
              "last_success": None,
              "product": None
          })
      entry["implemented"] = True
      if record.last_run:
        if (not entry["last_run"] or record.last_run > entry["last_run"]):
          entry["last_run"] = record.last_run
          entry["last_run_status"] = record.last_run_status
      if record.last_success:
        if (not entry["last_success"] or
            record.last_success > entry["last_success"]):
          entry["last_success"] = record.last_success
      csv_flag = product_status.get(record.requirement_id)
      product_flag = _combine_product_flags(csv_flag,
                                            record.product_implemented)
      entry["product"] = _merge_product_flags(entry["product"], product_flag)

  for use_case in use_cases.values():
    user_story_to_usecases[use_case.user_story_id].append(use_case)

  rows: List[List[str]] = []
  for user_story in user_story_ids:
    entries = sorted(user_story_to_usecases.get(user_story, []),
                     key=lambda uc: uc.anchor_id)
    if not entries:
      rows.append([
        _format_reference(user_story),
        "-",
        "-",
        "-",
        "-",
        "-",
      ])
      continue
    user_story_cell = _format_reference(user_story)
    for use_case in entries:
      use_case_anchor = use_case.anchor_id
      test_aspect_refs = ta_usage_by_usecase.get(use_case_anchor, {})
      if not test_aspect_refs:
        rows.append([
          user_story_cell,
          _format_reference(use_case_anchor),
          "-",
          "-",
          "-",
          "-",
        ])
        continue
      for test_aspect, flags in sorted(test_aspect_refs.items()):
        last_run = _format_status_cell(flags.get("last_run_status"),
                                       flags.get("last_run"))
        rows.append([
          user_story_cell,
          _format_reference(use_case_anchor),
          _format_reference(test_aspect),
          _format_optional_boolean(flags.get("product")),
          _format_boolean(flags.get("implemented", False)),
          last_run,
        ])

  headers = [
    "User Story",
    "Use Case",
    "Testaspekt",
    "im Produkt umgesetzt",
    "implementiert (Szenario vorhanden)",
    "letzter Teststatus",
  ]
  _suppress_repeated_cells(rows, (0, 1))
  return _write_asciidoc_table(
      headers,
      rows,
      disclaimer=ASCIIDOC_DISCLAIMER,
      cols_directive="1,1,2a,1,1,2",
  )


def _render_product_gap_table(
    requirements: Dict[str, Requirement],
    test_aspects: Dict[str, TestAspect],
    records: Sequence[TraceabilityRecord],
    product_status: Dict[str, Optional[str]],
    status_path: Optional[Path],
) -> Tuple[str, Dict[str, int]]:
  """Render product vs. testsuite coverage gaps per requirement."""
  entries, summary_counts = _build_gap_entries(requirements, test_aspects,
                                               records, product_status)

  rows: List[List[str]] = []
  for entry in entries:
    rows.append([
      str(entry["index"]),
      _format_reference(entry["requirement_id"]),
      _format_optional_boolean(entry["product_flag"]),
      entry["coverage_label"],
      entry["status"],
    ])

  headers = [
    "Nr.",
    "Anforderung",
    "umgesetzt",
    "getestet",
    "Hinweise",
  ]
  table = _write_asciidoc_table(
      headers,
      rows,
      disclaimer=ASCIIDOC_DISCLAIMER,
      cols_directive="1,1,1,1,2",
  )
  prefix: List[str] = []
  if status_path:
    prefix.append(f"// Produktstatus-Quelle: {status_path.as_posix()}")
  content = ("\n".join(prefix) + "\n" + table) if prefix else table
  if not content.endswith("\n"):
    content += "\n"
  return content, summary_counts


def _render_traceability_matrix(records: Sequence[TraceabilityRecord]) -> str:
  """Render a requirement → test-aspect → use-case matrix in Asciidoc."""
  requirements_grouped: Dict[
    str, Dict[str, List[TraceabilityRecord]]] = defaultdict(
      lambda: defaultdict(list)
  )
  rows: List[List[str]] = []
  for record in records:
    requirements_grouped[record.requirement_id][record.test_aspect_id].append(
        record)

  for requirement_id in sorted(requirements_grouped):
    test_aspect_groups = requirements_grouped[requirement_id]
    for test_aspect_id in sorted(test_aspect_groups):
      entries_sorted = sorted(
          test_aspect_groups[test_aspect_id],
          key=lambda rec: rec.use_case_id or "",
      )
      if not entries_sorted:
        entries_sorted = [
            TraceabilityRecord(
                requirement_id=requirement_id,
                test_aspect_id=test_aspect_id,
                use_case_id=None,
                implemented=False,
                product_implemented=False,
                last_run=None,
                last_run_status=None,
                last_success=None,
                scenario_names=set(),
          )
        ]
      for entry in entries_sorted:
        use_case = _format_reference(
            entry.use_case_id) if entry.use_case_id else "keiner"
        implemented = "ja" if entry.implemented else "nein"
        last_run_human = _format_status_cell(entry.last_run_status,
                                             entry.last_run)
        rows.append([
          _format_reference(requirement_id),
          _format_reference(test_aspect_id),
          use_case,
          implemented,
          last_run_human,
        ])

  headers = [
    "Anforderung",
    "Testaspekt",
    "Use Case",
    "implementiert",
    "letzter Teststatus",
  ]
  _suppress_repeated_cells(rows, (0, 1))
  return _write_asciidoc_table(
      headers,
      rows,
      disclaimer=ASCIIDOC_DISCLAIMER,
      cols_directive="1,2,1,2,2",
  )


def _render_product_gap_summary(summary_counts: Dict[str, int]) -> Tuple[str,
                                                                         str]:
  """Render a pie chart and table summarising product vs. test coverage."""

  chart_lines = [MERMAID_THEME_DIRECTIVE, "pie showData"]
  chart_lines.append(
      f'  "Umgesetzt & getestet" : {summary_counts["implemented_tested"]}')
  chart_lines.append("  "
                     f'"Umgesetzt, Tests fehlen/teilweise" : {summary_counts["implemented_missing_tests"]}'
                     )
  chart_lines.append(
      f'  "Getestet, Umsetzung unklar/nicht" : {summary_counts["tested_but_unimplemented"]}'
  )
  chart_lines.append(
      f'  "Ungetestet & Umsetzung unklar" : {summary_counts["untested_unknown"]}'
  )
  chart_lines.append("")
  chart = "\n".join(chart_lines)

  summary_rows = [
    ["Umgesetzt & getestet", str(summary_counts["implemented_tested"])],
    [
      "Umgesetzt, Tests fehlen/teilweise",
      str(summary_counts["implemented_missing_tests"])
    ],
    [
      "Getestet, Umsetzung unklar/nicht",
      str(summary_counts["tested_but_unimplemented"])
    ],
    [
      "Ungetestet & Umsetzung unklar",
      str(summary_counts["untested_unknown"])
    ],
  ]
  summary_table = _write_asciidoc_table(
      ["Kategorie", "Anzahl"],
      summary_rows,
      disclaimer=ASCIIDOC_DISCLAIMER,
      cols_directive="2,1",
  )
  return chart, summary_table


def _write_asciidoc_table(
    headers: Sequence[str],
    rows: Sequence[Sequence[str]],
    *,
    disclaimer: Optional[str] = None,
    cols_directive: str,
    block_attributes: Optional[Sequence[str]] = None,
    row_roles: Optional[Sequence[Optional[str]]] = None,
) -> str:
  """Return an Asciidoc table rendered from ``headers`` and ``rows``.

  The helper delegates cell formatting and escaping to ``pytablewriter`` so that
  nested commas or whitespace are handled consistently with other table
  producers in the toolchain.  Custom metadata such as the autogenerated
  disclaimer and the column width directive is injected on top, leaving the
  generated body untouched.
  """

  writer = AsciiDocTableWriter()
  writer.headers = list(headers)
  writer.value_matrix = [list(row) for row in rows] if rows else [["-"] *
                                                                  len(headers)]
  buffer = io.StringIO()
  writer.stream = buffer
  writer.write_table()
  table_body = buffer.getvalue().strip()

  if row_roles:
    table_body = _inject_row_roles(table_body, row_roles, len(headers))

  prefix_lines: List[str] = []
  if disclaimer:
    prefix_lines.append(disclaimer)
  if block_attributes:
    prefix_lines.extend(block_attributes)
  prefix_lines.append(f'[%header,cols="{cols_directive}",options="autowidth"]')

  content = "\n".join(prefix_lines + [table_body])
  if not content.endswith("\n"):
    content += "\n"
  return content


def _inject_row_roles(table_body: str, row_roles: Sequence[Optional[str]],
                      col_count: int) -> str:
  """Insert Asciidoc row roles before each data row."""
  lines = table_body.splitlines()
  output: List[str] = []
  data_row_index = 0
  header_lines_seen = 0
  in_data = False
  cols_seen = 0

  for line in lines:
    if line.startswith("^|"):
      header_lines_seen += 1
      output.append(line)
      continue
    if not in_data and header_lines_seen >= col_count and line.strip() == "":
      output.append(line)
      in_data = True
      continue
    if in_data and line.startswith("|"):
      if cols_seen % col_count == 0 and data_row_index < len(row_roles):
        role = row_roles[data_row_index]
        if role:
          output.append(f"[.{role}]")
        data_row_index += 1
      cols_seen += 1
      output.append(line)
      continue
    output.append(line)

  return "\n".join(output)


def _suppress_repeated_cells(
    rows: Sequence[Sequence[str]],
    columns: Sequence[int],
) -> None:
  """Blank out duplicate values in ``columns`` to visually group rows.

  Unlike traditional Asciidoc row spans this approach keeps the markup simple
  while still highlighting the hierarchical grouping (e.g. requirement → test
  aspect).  The first occurrence of each value remains intact and subsequent
  repetitions are replaced by empty strings.
  """
  if not rows:
    return
  for column in columns:
    previous: Optional[str] = None
    for row in rows:
      if column >= len(row):
        continue
      value = row[column]
      if previous is not None and value == previous:
        row[column] = ""
      else:
        previous = value


def _render_mermaid_diagram(
    records: Sequence[TraceabilityRecord],
    requirements: Dict[str, Requirement],
    test_aspects: Dict[str, TestAspect],
    use_cases: Dict[str, UseCase],
) -> str:
  """Render the traceability graph as Mermaid markup."""
  lines: List[str] = [
    MERMAID_DISCLAIMER,
    "%% Links requirements to their test aspects and, where available, the executing use cases.",
    MERMAID_THEME_DIRECTIVE,
    "graph LR",
  ]

  seen_nodes: Set[str] = set()
  edges: Set[Tuple[str, str]] = set()

  for record in records:
    req = requirements.get(record.requirement_id)
    ta = test_aspects.get(record.test_aspect_id)
    uc = use_cases.get(record.use_case_id or "", None)

    if req and req.requirement_id not in seen_nodes:
      label = _escape_mermaid_label(f"{req.requirement_id}<br>{req.title}")
      lines.append(f'  {req.requirement_id}["{label}"]')
      seen_nodes.add(req.requirement_id)

    if ta and ta.test_aspect_id not in seen_nodes:
      label = _escape_mermaid_label(f"{ta.test_aspect_id}<br>{ta.title}")
      lines.append(f'  {ta.test_aspect_id}["{label}"]')
      seen_nodes.add(ta.test_aspect_id)

    if uc and uc.anchor_id not in seen_nodes:
      label = _escape_mermaid_label(f"{uc.anchor_id}<br>{uc.title}")
      lines.append(f'  {uc.anchor_id}["{label}"]')
      seen_nodes.add(uc.anchor_id)

    if req and ta:
      edges.add((req.requirement_id, ta.test_aspect_id))
    if ta and uc:
      edges.add((ta.test_aspect_id, uc.anchor_id))

  for source, target in sorted(edges):
    lines.append(f"  {source} --> {target}")

  return "\n".join(lines) + "\n"


def _write_text_file(path: Path, content: str) -> None:
  """Write ``content`` to ``path`` only when changes are detected."""
  existing = path.read_text(encoding="utf-8") if path.exists() else None
  if existing == content:
    LOGGER.debug("No changes for %s", path)
    return
  path.write_text(content, encoding="utf-8")
  LOGGER.info("Wrote %s", path)


def _extract_title(path: Path, pattern: re.Pattern, *, fallback: str) -> str:
  """Return the first heading matching ``pattern`` or ``fallback``."""
  for line in path.read_text(encoding="utf-8").splitlines():
    match = pattern.match(line.strip())
    if match:
      return match.group("title").strip()
  return fallback


def _extract_anchor(path: Path) -> Optional[str]:
  """Return the first Asciidoc anchor in ``path`` if present."""
  if not path.exists():
    return None
  for line in path.read_text(encoding="utf-8").splitlines():
    match = ANCHOR_PATTERN.match(line.strip())
    if match:
      return match.group("anchor")
  return None


def _infer_use_case_from_path(relative_feature_path: Path) -> Optional[str]:
  """Infer a use-case tag from the feature path structure."""
  for part in relative_feature_path.parts:
    if part.startswith("UseCase_"):
      return part
  return None


def _story_key_from_path(relative_feature_path: Path) -> Optional[str]:
  """Infer the user-story directory name from a feature path."""
  parts = list(relative_feature_path.parts)
  for part in parts:
    if part.startswith("UserStory_"):
      return part
  if "userstories" not in parts:
    return None
  idx = parts.index("userstories")
  if idx + 1 < len(parts):
    return parts[idx + 1]
  return None


def _has_product_implementation_tag(tags: Set[str]) -> bool:
  """Backward compatibility shim for legacy product_impl tags."""
  return any(tag.lower() in {"product_impl", "produkt_impl"} for tag in tags)


def _has_not_implemented_tag(tags: Set[str]) -> bool:
  """Detect whether a scenario/feature is flagged as not implemented in product."""
  if _has_product_implementation_tag(tags):
    return False
  return any(tag.lower() in PRODUCT_NOT_IMPLEMENTED_TAGS for tag in tags)


def _escape_mermaid_label(label: str) -> str:
  """Sanitise Mermaid node labels to avoid syntax issues."""
  sanitised = label.replace("\n", "<br>")
  sanitised = sanitised.replace('"', "&quot;")
  return sanitised


def _format_reference(identifier: Optional[str]) -> str:
  """Wrap identifiers as Asciidoc anchors when possible."""
  if not identifier:
    return "-"
  if identifier == "UNSPECIFIED" or " " in identifier:
    return identifier
  return f"<<{identifier}>>"


def _format_boolean(value: bool) -> str:
  """Render boolean flags consistently for Asciidoc tables."""
  return "ja" if value else "nein"


def _format_optional_boolean(value: Optional[object]) -> str:
  """Render optional boolean-like flags with an explicit unknown state."""
  if value is None:
    return "unbekannt"
  if isinstance(value, str):
    lowered = value.strip().lower()
    if lowered in {"ja", "yes", "true"}:
      return "ja"
    if lowered in {"nein", "no", "false"}:
      return "nein"
    if lowered == "teilweise":
      return "teilweise"
  if isinstance(value, bool):
    return _format_boolean(value)
  return str(value)


def _normalise_product_flag(raw_value: str) -> Optional[str]:
  """Map free-form CSV entries to ``ja``/``nein``/``teilweise``."""
  value = raw_value.strip().lower()
  if not value:
    return None
  if value in {"ja", "yes", "true", "1", "y"}:
    return "ja"
  if value in {"nein", "no", "false", "0", "n"}:
    return "nein"
  if value in {"teilweise", "partial", "partially", "teilw."}:
    return "teilweise"
  return None


def _combine_product_flags(
    csv_flag: Optional[str],
    scenario_flag: bool,
) -> Optional[str]:
  """Combine product info from CSV with scenario-level tags.

  CSV feedback wins when present: explicit ``False`` stays ``False`` and
  explicit ``True`` is refined by the scenario flag. When no CSV data exists
  the scenario tag is used so product_not_impl markings still drive the column.
  """
  if csv_flag is None:
    return "ja" if scenario_flag else "nein"
  if csv_flag == "nein":
    return "nein"
  if csv_flag == "teilweise":
    return "teilweise" if scenario_flag else "nein"
  return "ja" if scenario_flag else "nein"


def _merge_product_flags(
    current: Optional[str],
    incoming: Optional[str],
) -> Optional[str]:
  """Accumulate product flags across multiple records.

  Unknown (``None``) never overrides existing values; otherwise we keep the
  most conservative state (``nein`` < ``teilweise`` < ``ja``) to avoid claiming
  product availability if any linked scenario indicates the opposite.
  """
  if incoming is None:
    return current
  if current is None:
    return incoming
  ordering = {"nein": 0, "teilweise": 1, "ja": 2}
  current_score = ordering.get(current, 2)
  incoming_score = ordering.get(incoming, 2)
  return current if current_score <= incoming_score else incoming


def _build_gap_entries(
    requirements: Dict[str, Requirement],
    test_aspects: Dict[str, TestAspect],
    records: Sequence[TraceabilityRecord],
    product_status: Dict[str, Optional[str]],
) -> Tuple[List[Dict[str, object]], Dict[str, int]]:
  """Derive gap entries with coverage labels, statuses and summary counts."""
  requirement_to_tas: Dict[str, Set[str]] = defaultdict(set)
  for ta in test_aspects.values():
    requirement_to_tas[ta.requirement_id].add(ta.test_aspect_id)

  testsuite_coverage: Dict[str, Set[str]] = defaultdict(set)
  for record in records:
    if record.implemented:
      testsuite_coverage[record.requirement_id].add(record.test_aspect_id)

  entries: List[Dict[str, object]] = []
  summary_counts = {
    "implemented_tested": 0,
    "implemented_missing_tests": 0,
    "tested_but_unimplemented": 0,
    "untested_unknown": 0,
  }

  for idx, requirement_id in enumerate(sorted(requirements), start=1):
    product_flag = product_status.get(requirement_id)
    tas_total = len(requirement_to_tas.get(requirement_id, set()))
    tas_covered = len(testsuite_coverage.get(requirement_id, set()))

    if tas_total == 0:
      coverage_label = "nein"
    elif tas_covered == 0:
      coverage_label = "nein"
    elif tas_covered < tas_total:
      coverage_label = "teilweise"
    else:
      coverage_label = "ja"

    if product_flag == "ja" and coverage_label == "ja":
      status = "OK"
      summary_counts["implemented_tested"] += 1
    elif product_flag == "ja" and coverage_label == "teilweise":
      status = "Teilweise getestet"
      summary_counts["implemented_missing_tests"] += 1
    elif product_flag == "ja":
      status = "Tests fehlen"
      summary_counts["implemented_missing_tests"] += 1
    elif product_flag == "teilweise" and coverage_label == "ja":
      status = "Produkt teilweise umgesetzt, Tests vorhanden"
      summary_counts["tested_but_unimplemented"] += 1
    elif product_flag == "teilweise" and coverage_label == "teilweise":
      status = "Produkt teilweise umgesetzt, teilweise getestet"
      summary_counts["tested_but_unimplemented"] += 1
    elif product_flag == "teilweise":
      status = "Produkt teilweise umgesetzt, Tests fehlen"
      summary_counts["implemented_missing_tests"] += 1
    elif product_flag == "nein" and coverage_label == "ja":
      status = "Produkt fehlt (Tests vorhanden)"
      summary_counts["tested_but_unimplemented"] += 1
    elif product_flag == "nein" and coverage_label == "teilweise":
      status = "Produkt fehlt (teilweise getestet)"
      summary_counts["tested_but_unimplemented"] += 1
    elif product_flag == "nein":
      status = "Produkt fehlt & ungetestet"
      summary_counts["untested_unknown"] += 1
    elif product_flag is None and coverage_label != "nein":
      status = "Produkt unbekannt (Tests vorhanden)"
      summary_counts["tested_but_unimplemented"] += 1
    else:
      status = "Keine Angaben"
      summary_counts["untested_unknown"] += 1

    entries.append({
      "index": idx,
      "requirement_id": requirement_id,
      "product_flag": product_flag,
      "coverage_label": coverage_label,
      "status": status,
    })

  return entries, summary_counts


def _resolve_execution_sources(root: Path, override: Optional[Path]) -> List[
  Path]:
  """Collect candidate JSON artefacts with execution information.

  The resolver honours an explicit override first and otherwise probes a
  number of default Cucumber and Serenity output locations. Directories are
  preserved so the caller can decide how much detail to parse from them.
  """

  candidates: List[Path] = []
  checked: List[Path] = []

  if override is not None:
    resolved = override if override.is_absolute() else root / override
    if resolved.exists():
      LOGGER.debug("Using execution artefact from explicit path %s", resolved)
      candidates.append(resolved)
    else:
      checked.append(resolved)

  default_relatives = [
    Path("target/cucumber/traceability.json"),
    Path("target/cucumber/cucumber.json"),
    Path("target/cucumber.json"),
    Path("target/cucumber-report/traceability.json"),
    Path("target/cucumber-report/cucumber.json"),
    Path("target/cucumber-reports/traceability.json"),
    Path("target/cucumber-reports/cucumber.json"),
    Path("target/site/serenity/serenity-summary.json"),
  ]

  for relative in default_relatives:
    candidate = root / relative
    if candidate.exists():
      candidates.append(candidate)
    else:
      checked.append(candidate)

  # If no direct file match was found, include well-known directories so the
  # loader can inspect individual scenario files (e.g. Serenity per-test JSON).
  default_directories = [
    root / "target" / "cucumber",
    root / "target" / "cucumber-report",
    root / "target" / "cucumber-reports",
    root / "target" / "site" / "serenity",
  ]

  for directory in default_directories:
    if directory.exists():
      candidates.append(directory)
    else:
      checked.append(directory)

  if not candidates and checked:
    LOGGER.info(
        "No execution artefacts found. Checked: %s",
        ", ".join(str(candidate) for candidate in checked),
    )

  # Deduplicate while preserving order.
  seen: Set[Path] = set()
  deduped: List[Path] = []
  for candidate in candidates:
    if candidate not in seen:
      deduped.append(candidate)
      seen.add(candidate)
  return deduped


def _dataclass_to_payload(instance, *, path_fields: Tuple[str, ...] = ()) -> \
    Dict[str, object]:
  """Convert dataclass instances into JSON-serialisable dictionaries."""
  payload = asdict(instance)
  for field in path_fields:
    value = payload.get(field)
    if isinstance(value, Path):
      payload[field] = value.as_posix()
  return payload


def _build_use_case_tag(story_dir: str, use_case_dir: str) -> Optional[str]:
  """Build the combined ``UseCase_UserStory_XX_YY`` tag form."""
  if not story_dir.startswith("UserStory_") or not use_case_dir.startswith(
      "UseCase_"):
    return None
  try:
    story_suffix = story_dir.split("_", 1)[1]
    use_case_suffix = use_case_dir.split("_", 1)[1]
  except IndexError:
    return None
  if not story_suffix or not use_case_suffix:
    return None
  return f"UseCase_{story_suffix}_{use_case_suffix}"


def _render_coverage_charts(
    records: Sequence[TraceabilityRecord],
    requirements: Dict[str, Requirement],
    test_aspects: Dict[str, TestAspect],
) -> Tuple[Dict[str, str], Dict[str, Dict[str, int]]]:
  """Build Mermaid diagrams and statistics for coverage summaries."""
  requirement_to_test_aspects: Dict[str, Set[str]] = defaultdict(set)
  for test_aspect in test_aspects.values():
    requirement_to_test_aspects[test_aspect.requirement_id].add(
        test_aspect.test_aspect_id)

  covered_test_aspects: Set[str] = {
    record.test_aspect_id for record in records if record.implemented
  }

  coverage_categories: Dict[str, int] = defaultdict(int)
  for requirement_id in requirements:
    aspects = requirement_to_test_aspects.get(requirement_id, set())
    if not aspects:
      coverage_categories["keine Testaspekte"] += 1
      continue
    covered = len(aspects & covered_test_aspects)
    if covered == 0:
      coverage_categories["nicht abgedeckt"] += 1
    elif covered == len(aspects):
      coverage_categories["vollständig abgedeckt"] += 1
    else:
      coverage_categories["teilweise abgedeckt"] += 1

  requirement_labels = (
    "vollständig abgedeckt",
    "teilweise abgedeckt",
    "nicht abgedeckt",
    "keine Testaspekte",
  )
  requirement_summary = {label: coverage_categories.get(label, 0) for label in
                         requirement_labels}
  requirement_summary["gesamt"] = sum(requirement_summary.values())

  total_test_aspects = len(test_aspects)
  implemented_test_aspects = {
    ta_id
    for ta_id in test_aspects
    if any(record.implemented for record in records if
           record.test_aspect_id == ta_id)
  }
  open_test_aspects = max(total_test_aspects - len(implemented_test_aspects), 0)
  test_aspect_summary = {
    "implementiert": len(implemented_test_aspects),
    "offen": open_test_aspects,
    "gesamt": total_test_aspects,
  }

  requirement_pie: List[str] = [
    MERMAID_DISCLAIMER,
    "%% Anteil der Anforderungen je Abdeckungsstatus basierend auf implementierten Testaspekten.",
    MERMAID_THEME_DIRECTIVE,
    "pie showData",
    '  title Anforderungen nach Abdeckungsstatus',
  ]
  for label in requirement_labels:
    count = requirement_summary[label]
    if count:
      requirement_pie.append(f'  "{label}" : {count}')

  test_aspect_pie: List[str] = [
    MERMAID_DISCLAIMER,
    "%% Gegenüberstellung implementierter und offener Testaspekte.",
    MERMAID_THEME_DIRECTIVE,
    "pie showData",
    '  title Testaspekte nach Implementierungsstatus',
  ]
  for label in ("implementiert", "offen"):
    count = test_aspect_summary[label]
    if count:
      test_aspect_pie.append(f'  "{label}" : {count}')

  diagrams = {
    "traceability-coverage-requirements.mmd": "\n".join(requirement_pie) + "\n",
    "traceability-coverage-testaspects.mmd": "\n".join(test_aspect_pie) + "\n",
  }
  return diagrams, {
    "requirements": requirement_summary,
    "test_aspects": test_aspect_summary,
  }


def _humanize_timestamps(
    records: Sequence[TraceabilityRecord],
    attribute: str,
) -> Dict[Tuple[str, str], Optional[str]]:
  """Format execution timestamps (e.g. last_run/last_success) as readable strings."""

  humanized: Dict[Tuple[str, str], Optional[str]] = {}
  for record in records:
    key = (record.test_aspect_id, record.use_case_id or "")
    value = getattr(record, attribute, None)
    humanized[key] = _format_human_timestamp(value)
  return humanized


def _format_timestamp_iso(value: Optional[datetime]) -> Optional[str]:
  """Return an ISO-8601 timestamp in UTC (``...Z``) or ``None``."""

  if value is None:
    return None

  timestamp = value
  if timestamp.tzinfo is None:
    timestamp = timestamp.replace(tzinfo=timezone.utc)
  utc_value = timestamp.astimezone(timezone.utc)
  return utc_value.isoformat(timespec="seconds").replace("+00:00", "Z")


def _format_human_timestamp(value: Optional[datetime]) -> Optional[str]:
  """Render timestamps in a friendly localised format."""

  if value is None:
    return None

  timestamp = value
  if timestamp.tzinfo is None:
    timestamp = timestamp.replace(tzinfo=timezone.utc)

  return timestamp.astimezone().strftime("%Y-%m-%d %H:%M:%S")


def _format_status_cell(status: Optional[str],
                        timestamp: Optional[datetime]) -> str:
  """Render a combined status + timestamp cell for tables."""
  if status is None:
    if timestamp is None:
      return "nie"
    status = "unknown"
  label_map = {
    "passed": "bestanden",
    "failed": "fehlgeschlagen",
    "skipped": "übersprungen",
    "unknown": "unbekannt",
  }
  label = label_map.get(status, status)
  human_time = _format_human_timestamp(timestamp)
  if human_time:
    return f"{label} ({human_time})"
  return label
